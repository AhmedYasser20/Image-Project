# -*- coding: utf-8 -*-
"""notebook5197ac4ac7

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebook5197ac4ac7-29651ff6-74b8-4656-b0e4-6b40e56628d1.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241221/auto/storage/goog4_request%26X-Goog-Date%3D20241221T131659Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D797d5b0022be8d94ac9ed2121df221e7d649ea189692c4df628a3420e0c567b9558c1d25425268c396eddc7c05c1f7d51789738fba5c36ead061d1893d21b7763dc5cf23eea3ca13b492b2cc5d2d0316eaffd78ad115cd4f8cb95f154688d25bab90792c643ea06ab289606f4a9fe025e272cd849dcce2e04a4bdb371995be8da883f735c24719e26c1992173c89a31f593e59ed2e5e40b22228002f9a3e9301f1e9849d5eb9bc2965184c63c5814fb3e76d3e6b21347a7feb3f7fb2dea6d324397e74c4e763381ca9c74aad7593516b73c46f34e35fa13528b25362a17cef848677182cc9607b7a53d35a803ca82eea116c3b2bfadf355ccc444c2a3448cf58
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

ahmedyasser147_music_project_path = kagglehub.dataset_download('ahmedyasser147/music-project')
ahmedyasser147_cleaneddataset_path = kagglehub.dataset_download('ahmedyasser147/cleaneddataset')
ahmedyasser147_dataset_cleaned_path = kagglehub.dataset_download('ahmedyasser147/dataset-cleaned')

print('Data source import complete.')

dataset_path = "/kaggle/input/cleaneddataset"  # Replace with the correct folder structure

import os
import numpy as np
import cupy as cp
import cv2
from skimage.io import imread
from sklearn.cluster import KMeans
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
import joblib
import traceback


def extract_hog_features(img,target_img_size):

    img = cv2.resize(img, target_img_size)
    win_size = (32, 32)
    cell_size = (4, 4)
    block_size_in_cells = (2, 2)

    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])
    block_stride = (cell_size[1], cell_size[0])
    nbins = 9  # Number of orientation bins
    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)
    h = hog.compute(img)
    h = h.flatten()
    return h.flatten()


# Load images and labels with improved error handling
def load_dataset(folder_path):
    X = []  # Features
    y = []  # Labels
    total_images = 0
    processed_images = 0


    for class_name in os.listdir(folder_path):
        class_path = os.path.join(folder_path, class_name)
        if os.path.isdir(class_path):

            for file_name in os.listdir(class_path):
                total_images += 1
                file_path = os.path.join(class_path, file_name)

                try:
                    # Check file extension and readability
                    if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                        image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)

                        # Skip invalid images
                        if image is None:
                            print(f"Skipping invalid image: {file_path}")
                            continue

                        # Extract descriptors
                        descriptors = extract_hog_features(image,(32, 32))

                        # Only add if descriptors are found
                        if descriptors is not None and len(descriptors) > 0:
                            X.append(descriptors)
                            y.append(class_name)
                            processed_images += 1
                        else:
                            print(f"No descriptors found for: {file_path}")

                except Exception as e:
                    print(f"Error processing {file_path}: {e}")


    return X, y



X, y = load_dataset(dataset_path)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)
print(classification_report(y_test, y_pred))

import joblib
joblib.dump(knn, "knn_model.pkl)
